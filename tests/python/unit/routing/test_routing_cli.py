"""reVRt routing CLI unit tests"""

import os
import json
import shutil
import platform
from pathlib import Path

import numpy as np
import pandas as pd
import pytest
import xarray as xr
import geopandas as gpd
import rasterio
from shapely.geometry import Point, LineString
from rasterio.transform import from_origin

from revrt._cli import main
from revrt.utilities import LayeredFile
from revrt.routing.utilities import map_to_costs
from revrt.exceptions import (
    revrtKeyError,
    revrtValueError,
    revrtFileNotFoundError,
)
from revrt.routing.cli import (
    compute_lcp_routes,
    build_routing_layer,
    build_route_costs_command,
    merge_output,
    _run_lcp,
    _collect_existing_routes,
    _update_multipliers,
    _route_points_subset,
    _paths_to_compute,
    _split_routes,
    _get_row_multiplier,
    _get_polarity_multiplier,
    _MILLION_USD_PER_MILE_TO_USD_PER_PIXEL,
)


@pytest.fixture(scope="module")
def sample_layered_data(tmp_path_factory):
    """Create layered routing data mimicking point_to_many tests"""

    data_dir = tmp_path_factory.mktemp("routing_cli_data")

    layered_fp = data_dir / "test_layered.zarr"
    layer_file = LayeredFile(layered_fp)

    height, width = (7, 8)
    cell_size = 1.0
    x0, y0 = 0.0, float(height)
    transform = from_origin(x0, y0, cell_size, cell_size)
    x_coords = (
        x0 + np.arange(width, dtype=np.float32) * cell_size + cell_size / 2
    )
    y_coords = (
        y0 - np.arange(height, dtype=np.float32) * cell_size - cell_size / 2
    )

    layer_values = [
        np.array(
            [
                [
                    [7, 7, 8, 0, 9, 9, 9, 0],
                    [8, 1, 2, 2, 9, 9, 9, 0],
                    [9, 1, 3, 3, 9, 1, 2, 3],
                    [9, 1, 2, 1, 9, 1, 9, 0],
                    [9, 9, 9, 1, 9, 1, 9, 0],
                    [9, 9, 9, 1, 1, 1, 9, 0],
                    [9, 9, 9, 9, 9, 9, 9, 0],
                ]
            ],
            dtype=np.float32,
        ),
        np.array(
            [
                [
                    [8, 7, 6, 5, 5, 6, 7, 9],
                    [7, 1, 1, 2, 3, 3, 2, 8],
                    [6, 2, 9, 6, 5, 2, 1, 7],
                    [7, 3, 8, 1, 2, 3, 2, 6],
                    [8, 4, 7, 2, 8, 4, 3, 5],
                    [9, 5, 6, 3, 4, 4, 3, 4],
                    [9, 6, 7, 4, 5, 5, 4, 3],
                ]
            ],
            dtype=np.float32,
        ),
        np.array(
            [
                [
                    [6, 6, 6, 6, 6, 7, 8, 9],
                    [5, 2, 2, 3, 4, 5, 6, 8],
                    [4, 3, 7, 7, 6, 4, 5, 7],
                    [5, 4, 6, 2, 3, 4, 4, 6],
                    [6, 5, 5, 3, 7, 5, 5, 5],
                    [7, 6, 6, 4, 5, 5, 4, 4],
                    [8, 7, 7, 5, 6, 5, 4, 3],
                ]
            ],
            dtype=np.float32,
        ),
    ]

    for ind, routing_layer in enumerate(layer_values, start=1):
        da = xr.DataArray(
            routing_layer,
            dims=("band", "y", "x"),
            coords={"y": y_coords, "x": x_coords},
        )
        da = da.rio.write_crs("EPSG:4326")
        da = da.rio.write_transform(transform)

        geotiff_fp = data_dir / f"layer_{ind}.tif"
        da.rio.to_raster(geotiff_fp, driver="GTiff")

        layer_file.write_geotiff_to_file(
            geotiff_fp, f"layer_{ind}", overwrite=True
        )

    return layered_fp


def _build_route_table(layered_fp, rows_cols):
    """Helper to construct route tables with CRS-aligned coordinates"""

    with xr.open_dataset(layered_fp, consolidated=False, engine="zarr") as ds:
        latitudes = ds["latitude"].to_numpy()
        longitudes = ds["longitude"].to_numpy()

    records = []
    for idx, (start, end) in enumerate(rows_cols):
        s_row, s_col = start
        e_row, e_col = end
        records.append(
            {
                "route_id": f"route_{idx}",
                "start_lat": float(latitudes[s_row, s_col]),
                "start_lon": float(longitudes[s_row, s_col]),
                "end_lat": float(latitudes[e_row, e_col]),
                "end_lon": float(longitudes[e_row, e_col]),
                "voltage": 138,
                "polarity": "ac",
            }
        )

    return pd.DataFrame.from_records(records)


def test_compute_lcp_routes_generates_csv(sample_layered_data, tmp_path):
    """compute_lcp_routes should map points and persist CSV outputs"""

    routes = _build_route_table(
        sample_layered_data,
        rows_cols=[((1, 1), (2, 3)), ((0, 0), (3, 4))],
    )
    route_table_fp = tmp_path / "route_table.csv"
    routes.to_csv(route_table_fp, index=False)

    out_dir = tmp_path / "routing_outputs"
    cost_layers = [
        {
            "layer_name": "layer_1",
            "apply_row_mult": True,
            "multiplier_scalar": 1,
        },
        {
            "layer_name": "layer_2",
            "apply_polarity_mult": True,
            "multiplier_scalar": 1,
        },
    ]

    transmission_config = {
        "row_width": {"138": 1.5},
        "voltage_polarity_mult": {"138": {"ac": 2e-5}},
    }

    result_fp = compute_lcp_routes(
        cost_fpath=sample_layered_data,
        route_table=route_table_fp,
        cost_layers=cost_layers,
        out_dir=out_dir,
        job_name="run",
        transmission_config=transmission_config,
        save_paths=False,
        _split_params=(0, 1),
    )

    output_path = Path(result_fp)
    assert output_path.exists()

    df = pd.read_csv(output_path)
    df = df[df["route_id"] != "route_id"].reset_index(drop=True)
    df = df.drop(columns=[c for c in df.columns if c.startswith("Unnamed")])

    numeric_cols = [
        "cost",
        "length_km",
        "optimized_objective",
        "layer_1_cost",
        "layer_1_dist_km",
        "layer_2_cost",
        "layer_2_dist_km",
    ]
    for col in numeric_cols:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col])

    assert len(df) == len(routes)
    assert set(df["route_id"]) == set(routes["route_id"])

    with xr.open_dataset(
        sample_layered_data, consolidated=False, engine="zarr"
    ) as ds:
        mapped_routes = map_to_costs(
            routes.copy(), ds.rio.crs, ds.rio.transform(), ds.rio.shape
        )

    merged = df.merge(
        mapped_routes[
            [
                "route_id",
                "start_row",
                "start_col",
                "end_row",
                "end_col",
            ]
        ],
        on="route_id",
        how="left",
        suffixes=("", "_expected"),
    )

    for col in ["start_row", "start_col", "end_row", "end_col"]:
        assert (
            merged[col].astype(int) == merged[f"{col}_expected"].astype(int)
        ).all()

    assert np.allclose(
        merged["cost"], merged["layer_1_cost"] + merged["layer_2_cost"]
    )
    assert np.all(merged["length_km"] > 0)
    assert np.allclose(
        merged["cost"], merged["optimized_objective"], rtol=1e-5
    )


def test_compute_lcp_routes_returns_none_on_empty_indices(
    sample_layered_data, tmp_path
):
    """Ensure compute_lcp_routes short-circuits when route points are empty"""

    route_table_fp = tmp_path / "routes.csv"
    _build_route_table(sample_layered_data, [((1, 1), (2, 2))]).to_csv(
        route_table_fp, index=False
    )

    result = compute_lcp_routes(
        cost_fpath=sample_layered_data,
        route_table=route_table_fp,
        cost_layers=[{"layer_name": "layer_1"}],
        out_dir=tmp_path,
        job_name="no_routes",
        _split_params=(1000, 1),
    )

    assert result is None
    assert not (tmp_path / "no_routes.csv").exists()


def test_run_lcp_with_save_paths_filters_existing_routes(
    sample_layered_data, tmp_path, monkeypatch
):
    """_run_lcp should skip already processed routes and append geometries"""

    routes = _build_route_table(
        sample_layered_data,
        rows_cols=[((1, 1), (2, 2)), ((2, 2), (4, 4))],
    )

    with xr.open_dataset(
        sample_layered_data, consolidated=False, engine="zarr"
    ) as ds:
        mapped_routes = map_to_costs(
            routes.copy(), ds.rio.crs, ds.rio.transform(), ds.rio.shape
        )

    existing_tuple = (
        int(mapped_routes.iloc[0]["start_row"]),
        int(mapped_routes.iloc[0]["start_col"]),
        int(mapped_routes.iloc[0]["end_row"]),
        int(mapped_routes.iloc[0]["end_col"]),
        routes.iloc[0]["polarity"],
        str(routes.iloc[0]["voltage"]),
    )

    monkeypatch.setattr(
        "revrt.routing.cli._collect_existing_routes",
        lambda _: {existing_tuple},
    )

    saved_calls = []

    def fake_to_file(self, path, driver=None, mode=None, **_kwargs):
        saved_calls.append((path, driver, mode, self.copy(deep=True)))

    monkeypatch.setattr(
        "revrt.routing.cli.gpd.GeoDataFrame.to_file", fake_to_file
    )

    out_fp = tmp_path / "routes.gpkg"

    _run_lcp(
        cost_fpath=sample_layered_data,
        route_points=routes,
        cost_layers=[{"layer_name": "layer_1"}],
        out_fp=out_fp,
        transmission_config={
            "row_width": {"138": 1.0},
            "voltage_polarity_mult": {"138": {"ac": 1.0}},
        },
        cost_multiplier_scalar=1,
        friction_layers=[{"mask": "layer_2", "apply_row_mult": True}],
        tracked_layers={"layer_3": "max"},
        ignore_invalid_costs=True,
    )

    assert len(saved_calls) == 1
    saved_path, driver, mode, saved_gdf = saved_calls[0]
    assert saved_path == out_fp
    assert driver == "GPKG"
    assert mode == "a"
    assert len(saved_gdf) == 1
    assert saved_gdf["route_id"].iloc[0] == routes.iloc[1]["route_id"]

    expected = mapped_routes.iloc[1]
    assert int(saved_gdf["start_row"].iloc[0]) == int(expected["start_row"])
    assert int(saved_gdf["start_col"].iloc[0]) == int(expected["start_col"])
    assert int(saved_gdf["end_row"].iloc[0]) == int(expected["end_row"])
    assert int(saved_gdf["end_col"].iloc[0]) == int(expected["end_col"])

    cost_val = float(saved_gdf["cost"].iloc[0])
    objective_val = float(saved_gdf["optimized_objective"].iloc[0])
    length_val = float(saved_gdf["length_km"].iloc[0])

    assert cost_val > 0
    assert length_val > 0
    assert objective_val > cost_val

    geom = saved_gdf.geometry.iloc[0]
    assert isinstance(geom, LineString)
    assert len(geom.coords) >= 2


def test_run_lcp_returns_immediately_when_no_routes(tmp_path):
    """_run_lcp should exit early when route_points is empty"""

    _run_lcp(
        cost_fpath="unused",  # cost file is ignored in this branch
        route_points=pd.DataFrame(),
        cost_layers=[],
        out_fp=tmp_path / "unused.csv",
    )


def test_collect_existing_routes_csv(tmp_path):
    """_collect_existing_routes should read CSV outputs"""

    data = pd.DataFrame(
        [
            {
                "start_row": 0,
                "start_col": 1,
                "end_row": 2,
                "end_col": 3,
                "polarity": "ac",
                "voltage": "230",
            }
        ]
    )
    csv_fp = tmp_path / "routes.csv"
    data.to_csv(csv_fp, index=False)

    result = _collect_existing_routes(csv_fp)
    assert result == {(0, 1, 2, 3, "ac", "230")}


def test_collect_existing_routes_gpkg(tmp_path):
    """_collect_existing_routes should support GeoPackage outputs"""

    gpkg_fp = tmp_path / "routes.gpkg"
    gdf = gpd.GeoDataFrame(
        {
            "start_row": [1],
            "start_col": [2],
            "end_row": [3],
            "end_col": [4],
            "polarity": ["unknown"],
            "voltage": ["unknown"],
            "geometry": [Point(0, 0)],
        }
    )
    gdf.to_file(gpkg_fp, driver="GPKG")

    result = _collect_existing_routes(gpkg_fp)
    assert result == {(1, 2, 3, 4, "unknown", "unknown")}


def test_collect_existing_routes_when_missing(tmp_path):
    """Missing outputs should result in an empty existing route set"""

    assert _collect_existing_routes(None) == set()
    assert _collect_existing_routes(tmp_path / "missing.csv") == set()


def test_route_points_subset_with_chunking(tmp_path):
    """_route_points_subset should slice sorted features by chunk"""

    test_fp = tmp_path / "features.csv"
    features = pd.DataFrame(
        {
            "start_lat": [5.0, 1.0, 3.0, 7.0],
            "start_lon": [0.0, 1.0, 2.0, 3.0],
        }
    )

    features.to_csv(test_fp, index=False)

    first_chunk = _route_points_subset(
        test_fp, ["start_lat", "start_lon"], (0, 2)
    )
    assert first_chunk["start_lat"].tolist() == [1.0, 3.0]
    assert first_chunk["start_lon"].tolist() == [1.0, 2.0]

    second_chunk = _route_points_subset(
        test_fp, ["start_lat", "start_lon"], (1, 2)
    )
    assert second_chunk["start_lat"].tolist() == [5.0, 7.0]
    assert second_chunk["start_lon"].tolist() == [0.0, 3.0]


def test_paths_to_compute_inserts_missing_columns(tmp_path):
    """_paths_to_compute should back-fill missing polarity/voltage columns"""

    route_points = pd.DataFrame(
        {
            "start_row": [0],
            "start_col": [1],
            "end_row": [2],
            "end_col": [3],
        }
    )

    groups = list(_paths_to_compute(route_points, tmp_path / "not_there.csv"))
    assert groups
    polarity, voltage, grouped_routes = groups[0]
    assert polarity == "unknown"
    assert voltage == "unknown"
    assert grouped_routes.iloc[0]["start_row"] == 0


def test_split_routes_handles_local_and_cluster():
    """_split_routes should configure chunking for local and cluster modes"""

    local_config = {"execution_control": {"option": "local", "nodes": 4}}
    result_local = _split_routes(local_config)
    assert result_local["_split_params"] == [(0, 1)]
    assert result_local["execution_control"]["nodes"] == 4

    cluster_config = {"execution_control": {"nodes": 3}}
    result_cluster = _split_routes(cluster_config)
    assert result_cluster["_split_params"] == [(0, 3), (1, 3), (2, 3)]
    assert "nodes" not in result_cluster["execution_control"]


def test_update_multipliers_applies_row_and_polarity():
    """_update_multipliers should apply configured scalar adjustments"""

    layers = [
        {
            "layer_name": "layer_1",
            "multiplier_scalar": 2,
            "apply_row_mult": True,
        },
        {"layer_name": "layer_2", "apply_polarity_mult": True},
    ]

    transmission_config = {
        "row_width": {"138": 1.5},
        "voltage_polarity_mult": {"138": {"ac": 0.5}},
    }

    updated = _update_multipliers(
        layers,
        polarity="ac",
        voltage=138,
        transmission_config=transmission_config,
    )

    # original input remains unchanged
    assert layers[0]["apply_row_mult"] is True
    assert updated[0]["multiplier_scalar"] == pytest.approx(3)
    assert updated[1]["multiplier_scalar"] == pytest.approx(
        0.5 * _MILLION_USD_PER_MILE_TO_USD_PER_PIXEL
    )

    # Voltage marked as unknown should skip multiplier lookups
    unchanged = _update_multipliers(
        [{"layer_name": "layer_3"}], "dc", "unknown", transmission_config
    )
    assert unchanged[0]["layer_name"] == "layer_3"


def test_get_row_multiplier_missing_config():
    """_get_row_multiplier should raise when configuration keys are absent"""

    with pytest.raises(
        revrtKeyError,
        match=(
            r"`apply_row_mult` was set to `True`, but 'row_width' not found "
            r"in transmission config"
        ),
    ):
        _get_row_multiplier({}, "138")


def test_get_row_multiplier_unknown_voltage():
    """_get_row_multiplier should surface available voltages on failure"""

    config = {"row_width": {"230": 1.2}}
    with pytest.raises(
        revrtKeyError,
        match=(
            r"`apply_row_mult` was set to `True`, but voltage '\s*138' not "
            r"found in transmission config 'row_width' settings. "
            r"Available voltages: \['230'\]"
        ),
    ):
        _get_row_multiplier(config, "138")


def test_get_polarity_multiplier_missing_config():
    """_get_polarity_multiplier should raise when multiplier section missing"""

    with pytest.raises(
        revrtKeyError,
        match=(
            r"`apply_polarity_mult` was set to `True`, but "
            r"'voltage_polarity_mult' not found in transmission config"
        ),
    ):
        _get_polarity_multiplier({}, "138", "ac")


def test_get_polarity_multiplier_unknown_voltage():
    """_get_polarity_multiplier should guard against unknown voltages"""

    config = {"voltage_polarity_mult": {"230": {"ac": 1.0}}}
    with pytest.raises(
        revrtKeyError,
        match=(
            r"`apply_polarity_mult` was set to `True`, but voltage '\s*138' "
            r"not found in polarity config. Available voltages: \['230'\]"
        ),
    ):
        _get_polarity_multiplier(config, "138", "ac")


def test_merge_routes_bad_collect_pattern(tmp_path):
    """merge_output should raise when collect pattern lacks wildcard"""
    with pytest.raises(
        revrtValueError, match="Collect pattern has no wildcard"
    ):
        merge_output(
            collect_pattern="no_wildcard_here.csv", project_dir=tmp_path
        )


def test_merge_routes_no_files(tmp_path):
    """merge_output should raise when no files match collect pattern"""
    with pytest.raises(
        revrtFileNotFoundError, match="No files found using collect pattern:"
    ):
        merge_output(collect_pattern="dne*.csv", project_dir=tmp_path)


@pytest.mark.skipif(
    (os.environ.get("TOX_RUNNING") == "True")
    and (platform.system() == "Windows"),
    reason="CLI does not work under tox env on windows",
)
def test_cli_collect_routes_merges_csv(cli_runner, tmp_path):
    """collect-routes CLI should merge CSV chunk outputs"""

    chunk_dir = tmp_path / "outputs"
    chunk_dir.mkdir(parents=True, exist_ok=True)

    chunk_frames = [
        pd.DataFrame(
            {
                "route_id": ["chunk0_route0", "chunk0_route1"],
                "start_row": [0, 1],
                "start_col": [0, 1],
                "end_row": [2, 3],
                "end_col": [2, 3],
                "cost": [10.0, 20.0],
            }
        ),
        pd.DataFrame(
            {
                "route_id": ["chunk1_route0", "chunk1_route1"],
                "start_row": [4, 5],
                "start_col": [4, 5],
                "end_row": [6, 7],
                "end_col": [6, 7],
                "cost": [30.0, 40.0],
            }
        ),
    ]

    for idx, frame in enumerate(chunk_frames):
        frame.to_csv(chunk_dir / f"routes_part_{idx}.csv", index=False)

    chunk_fp = chunk_dir / "routes_part_999.csv"
    pd.DataFrame(columns=["route_id"]).to_csv(chunk_fp, index=False)

    config = {
        "collect_pattern": "outputs/routes_part_*.csv",
        "chunk_size": 1,
    }

    config_fp = tmp_path / "collect_csv_config.json"
    config_fp.write_text(json.dumps(config))

    result = cli_runner.invoke(main, ["collect-routes", "-c", str(config_fp)])
    assert result.exit_code == 0, result.output

    merged_fp = chunk_dir / "routes_part_.csv"
    assert merged_fp.exists()

    merged_df = pd.read_csv(merged_fp)
    expected_df = pd.concat(chunk_frames, ignore_index=True)
    pd.testing.assert_frame_equal(
        merged_df.sort_values("route_id").reset_index(drop=True),
        expected_df.sort_values("route_id").reset_index(drop=True),
    )

    chunk_files_dir = chunk_dir / "chunk_files"
    assert chunk_files_dir.exists()

    for idx in range(len(chunk_frames)):
        original_fp = chunk_dir / f"routes_part_{idx}.csv"
        relocated_fp = chunk_files_dir / f"routes_part_{idx}.csv"
        assert not original_fp.exists()
        assert relocated_fp.exists()

    original_fp = chunk_dir / "routes_part_999.csv"
    relocated_fp = chunk_files_dir / "routes_part_999.csv"
    assert not original_fp.exists()
    assert relocated_fp.exists()


@pytest.mark.skipif(
    (os.environ.get("TOX_RUNNING") == "True")
    and (platform.system() == "Windows"),
    reason="CLI does not work under tox env on windows",
)
@pytest.mark.parametrize("tol", [None, 0.01])
def test_cli_collect_routes_merges_gpkg(cli_runner, tmp_path, tol):
    """collect-routes CLI should merge GeoPackage chunk outputs"""

    chunk_dir = tmp_path / "geoms"
    chunk_dir.mkdir(parents=True, exist_ok=True)

    chunk_geometries = []
    for idx in range(2):
        gdf = gpd.GeoDataFrame(
            {
                "route_id": [
                    f"chunk{idx}_route0",
                    f"chunk{idx}_route1",
                ],
                "start_row": [idx, idx + 10],
                "start_col": [idx, idx + 10],
                "end_row": [idx + 20, idx + 30],
                "end_col": [idx + 20, idx + 30],
            },
            geometry=[
                LineString(
                    [
                        (idx, idx),
                        (idx + 0.05, idx + 0.02),
                        (idx + 1.0, idx + 0.8),
                    ]
                ),
                LineString(
                    [
                        (idx + 1.0, idx),
                        (idx + 1.2, idx + 0.3),
                        (idx + 2.0, idx + 1.0),
                    ]
                ),
            ],
            crs="EPSG:4326",
        )

        chunk_fp = chunk_dir / f"segment_{idx}.gpkg"
        gdf.to_file(chunk_fp, driver="GPKG")
        chunk_geometries.append(gdf)

    chunk_fp = chunk_dir / "segment_999.gpkg"
    gpd.GeoDataFrame(columns=["route_id"]).to_file(chunk_fp, driver="GPKG")

    config = {
        "collect_pattern": "geoms/segment_*.gpkg",
        "chunk_size": 1,
        "simplify_geo_tolerance": tol,
        "out_fp": str(tmp_path / "merged_routes.gpkg"),
        "purge_chunks": True,
    }

    config_fp = tmp_path / "collect_gpkg_config.json"
    config_fp.write_text(json.dumps(config))

    result = cli_runner.invoke(main, ["collect-routes", "-c", str(config_fp)])
    assert result.exit_code == 0, result.output

    merged_fp = tmp_path / "merged_routes.gpkg"
    assert merged_fp.exists()

    merged_gdf = gpd.read_file(merged_fp)
    expected_count = sum(len(gdf) for gdf in chunk_geometries)
    assert len(merged_gdf) == expected_count
    assert set(merged_gdf["route_id"]) == {
        "chunk0_route0",
        "chunk0_route1",
        "chunk1_route0",
        "chunk1_route1",
    }
    assert all(geom.geom_type == "LineString" for geom in merged_gdf.geometry)

    for idx in range(len(chunk_geometries)):
        assert not (chunk_dir / f"segment_{idx}.gpkg").exists()
    assert not (chunk_dir / "segment_999.gpkg").exists()
    assert not (chunk_dir / "chunk_files").exists()


@pytest.mark.skipif(
    (os.environ.get("TOX_RUNNING") == "True")
    and (platform.system() == "Windows"),
    reason="CLI does not work under tox env on windows",
)
def test_cli_route_points_skips_precomputed_routes(
    cli_runner, sample_layered_data, tmp_path
):
    """route-points CLI should append only new routes"""

    routes = _build_route_table(
        sample_layered_data,
        rows_cols=[((1, 1), (2, 3)), ((0, 0), (3, 4))],
    )

    route_table_fp = tmp_path / "route_points.csv"
    routes.iloc[:1].to_csv(route_table_fp, index=False)

    config = {
        "cost_fpath": str(sample_layered_data),
        "route_table": str(route_table_fp),
        "cost_layers": [{"layer_name": "layer_1"}],
    }
    config_fp = tmp_path / "route_points_config.json"
    config_fp.write_text(json.dumps(config))

    first_result = cli_runner.invoke(
        main, ["route-points", "-c", str(config_fp)]
    )
    assert first_result.exit_code == 0, first_result.output

    out_fp = list(tmp_path.glob("*test*.csv"))
    assert len(out_fp) == 1
    out_fp = out_fp[0]
    assert out_fp.exists()

    first_run = pd.read_csv(out_fp)
    assert len(first_run) == 1

    base_route_id = routes.iloc[0]["route_id"]
    assert first_run["route_id"].tolist() == [base_route_id]
    first_row = first_run.iloc[0]

    sentinel_length = -4321.0
    raw_first = pd.read_csv(out_fp)
    assert "length_km" in raw_first.columns
    raw_first.loc[
        raw_first["route_id"].astype(str) == str(base_route_id),
        "length_km",
    ] = sentinel_length
    raw_first.to_csv(out_fp, index=False)

    routes.to_csv(route_table_fp, index=False)

    shutil.rmtree(tmp_path / ".gaps")
    second_result = cli_runner.invoke(
        main, ["route-points", "-c", str(config_fp)]
    )
    assert second_result.exit_code == 0, second_result.output

    assert len(list(tmp_path.glob("*test*.csv"))) == 1
    all_routes = pd.read_csv(out_fp)
    assert len(all_routes) == 2

    counts = all_routes["route_id"].value_counts()
    for route_id in routes["route_id"]:
        assert counts[route_id] == 1

    first_after = all_routes.loc[all_routes["route_id"] == base_route_id].iloc[
        0
    ]

    for col in ["cost", "optimized_objective"]:
        assert first_after[col] == pytest.approx(first_row[col])

    if "length_km" in all_routes.columns:
        assert first_after["length_km"] == pytest.approx(sentinel_length)

    with xr.open_dataset(
        sample_layered_data, consolidated=False, engine="zarr"
    ) as ds:
        mapped = map_to_costs(
            routes.copy(), ds.rio.crs, ds.rio.transform(), ds.rio.shape
        )

    merged = all_routes.merge(
        mapped[
            [
                "route_id",
                "start_row",
                "start_col",
                "end_row",
                "end_col",
            ]
        ],
        on="route_id",
        how="left",
        suffixes=("", "_expected"),
    )

    for col in ["start_row", "start_col", "end_row", "end_col"]:
        assert (
            merged[col].astype(int) == merged[f"{col}_expected"].astype(int)
        ).all()


@pytest.mark.skipif(
    (os.environ.get("TOX_RUNNING") == "True")
    and (platform.system() == "Windows"),
    reason="CLI does not work under tox env on windows",
)
def test_cli_route_points_skips_precomputed_routes_gpkg(
    cli_runner, sample_layered_data, tmp_path
):
    """route-points CLI should append only new routes (GPKG)"""

    routes = _build_route_table(
        sample_layered_data,
        rows_cols=[((1, 1), (2, 3)), ((0, 0), (3, 4))],
    )

    route_table_fp = tmp_path / "route_points.csv"
    routes.iloc[:1].to_csv(route_table_fp, index=False)

    config = {
        "cost_fpath": str(sample_layered_data),
        "route_table": str(route_table_fp),
        "cost_layers": [{"layer_name": "layer_1"}],
        "save_paths": True,
    }
    config_fp = tmp_path / "route_points_config.json"
    config_fp.write_text(json.dumps(config))

    first_result = cli_runner.invoke(
        main, ["route-points", "-c", str(config_fp)]
    )
    assert first_result.exit_code == 0, first_result.output

    out_fp = list(tmp_path.glob("*test*.gpkg"))
    assert len(out_fp) == 1
    out_fp = out_fp[0]
    assert out_fp.exists()

    first_run = gpd.read_file(out_fp)
    assert len(first_run) == 1

    base_route_id = routes.iloc[0]["route_id"]
    assert first_run["route_id"].tolist() == [base_route_id]
    first_row = first_run.iloc[0]

    sentinel_length = -4321.0
    raw_first = gpd.read_file(out_fp)
    assert "length_km" in raw_first.columns
    raw_first.loc[
        raw_first["route_id"].astype(str) == str(base_route_id),
        "length_km",
    ] = sentinel_length
    raw_first.to_file(out_fp, index=False, driver="GPKG")

    routes.to_csv(route_table_fp, index=False)

    shutil.rmtree(tmp_path / ".gaps")
    second_result = cli_runner.invoke(
        main, ["route-points", "-c", str(config_fp)]
    )
    assert second_result.exit_code == 0, second_result.output

    assert len(list(tmp_path.glob("*test*.gpkg"))) == 1
    all_routes = gpd.read_file(out_fp)
    assert len(all_routes) == 2

    counts = all_routes["route_id"].value_counts()
    for route_id in routes["route_id"]:
        assert counts[route_id] == 1

    first_after = all_routes.loc[all_routes["route_id"] == base_route_id].iloc[
        0
    ]

    for col in ["cost", "optimized_objective"]:
        assert first_after[col] == pytest.approx(first_row[col])

    if "length_km" in all_routes.columns:
        assert first_after["length_km"] == pytest.approx(sentinel_length)

    with xr.open_dataset(
        sample_layered_data, consolidated=False, engine="zarr"
    ) as ds:
        mapped = map_to_costs(
            routes.copy(), ds.rio.crs, ds.rio.transform(), ds.rio.shape
        )

    merged = all_routes.merge(
        mapped[
            [
                "route_id",
                "start_row",
                "start_col",
                "end_row",
                "end_col",
            ]
        ],
        on="route_id",
        how="left",
        suffixes=("", "_expected"),
    )

    for col in ["start_row", "start_col", "end_row", "end_col"]:
        assert (
            merged[col].astype(int) == merged[f"{col}_expected"].astype(int)
        ).all()


def test_get_polarity_multiplier_unknown_polarity():
    """_get_polarity_multiplier should guard against unknown polarities"""

    config = {"voltage_polarity_mult": {"138": {"dc": 1.0}}}
    with pytest.raises(
        revrtKeyError,
        match=(
            r"`apply_polarity_mult` was set to `True`, but polarity '\s*ac' "
            r"not found in voltage config. Available polarities: \['dc'\]"
        ),
    ):
        _get_polarity_multiplier(config, "138", "ac")


def test_build_route_costs_command_writes_expected_layers(
    sample_layered_data, tmp_path
):
    """build_route_costs_command should persist aggregated raster outputs"""

    config = {
        "cost_fpath": str(sample_layered_data),
        "cost_layers": [
            {"layer_name": "layer_1", "multiplier_scalar": 1.5},
            {"layer_name": "layer_2", "multiplier_scalar": 0.5},
        ],
        "cost_multiplier_scalar": 2.0,
        "ignore_invalid_costs": True,
    }

    config_fp = tmp_path / "lcp_config.json"
    config_fp.write_text(json.dumps(config))
    out_dir = tmp_path / "outputs"

    outputs = build_route_costs_command.runner(
        lcp_config_fp=config_fp,
        out_dir=out_dir,
        polarity=None,
        voltage=None,
    )

    assert len(outputs) == 2
    cost_fp, final_fp = [Path(fp) for fp in outputs]
    assert cost_fp.exists()
    assert final_fp.exists()

    with xr.open_dataset(
        sample_layered_data, consolidated=False, engine="zarr"
    ) as ds:
        layer_one = ds["layer_1"].isel(band=0).astype(np.float32).load()
        layer_two = ds["layer_2"].isel(band=0).astype(np.float32).load()

    expected_vals = (layer_one * 1.5 + layer_two * 0.5) * 2.0
    expected_vals = expected_vals.to_numpy()

    with rasterio.open(cost_fp) as src:
        agg_costs = src.read(1)

    with rasterio.open(final_fp) as src:
        final_layer = src.read(1)

    assert agg_costs.shape == expected_vals.shape
    assert final_layer.shape == expected_vals.shape
    assert np.allclose(agg_costs, expected_vals)
    assert np.allclose(final_layer, expected_vals)


@pytest.mark.skipif(
    (os.environ.get("TOX_RUNNING") == "True")
    and (platform.system() == "Windows"),
    reason="CLI does not work under tox env on windows",
)
def test_cli_build_route_costs_command(
    cli_runner, sample_layered_data, tmp_path
):
    """CLI build-route-costs command should produce routed rasters"""

    lcp_config = {
        "cost_fpath": str(sample_layered_data),
        "cost_layers": [
            {"layer_name": "layer_1", "multiplier_scalar": 1.5},
            {"layer_name": "layer_2", "multiplier_scalar": 0.5},
        ],
        "cost_multiplier_scalar": 2.0,
        "ignore_invalid_costs": True,
    }

    lcp_config_fp = tmp_path / "cli_lcp_config.json"
    lcp_config_fp.write_text(json.dumps(lcp_config))

    cli_config = {"lcp_config_fp": str(lcp_config_fp)}

    cli_config_fp = tmp_path / "cli_command_config.json"
    cli_config_fp.write_text(json.dumps(cli_config))

    result = cli_runner.invoke(
        main, ["build-route-costs", "-c", str(cli_config_fp)]
    )
    assert result.exit_code == 0, result.output

    cost_fp = tmp_path / "agg_costs.tif"
    final_fp = tmp_path / "final_routing_layer.tif"
    assert cost_fp.exists()
    assert final_fp.exists()

    with xr.open_dataset(
        sample_layered_data, consolidated=False, engine="zarr"
    ) as ds:
        layer_one = ds["layer_1"].isel(band=0).astype(np.float32).load()
        layer_two = ds["layer_2"].isel(band=0).astype(np.float32).load()

    expected_vals = (layer_one * 1.5 + layer_two * 0.5) * 2.0

    with rasterio.open(cost_fp) as src:
        agg_costs = src.read(1)

    with rasterio.open(final_fp) as src:
        final_layer = src.read(1)

    assert np.allclose(agg_costs, expected_vals)
    assert np.allclose(final_layer, expected_vals)


def test_build_route_costs_command_metadata():
    """build_route_costs_command should expose CLI settings"""

    assert build_route_costs_command.name == "build-route-costs"
    assert build_route_costs_command.runner is build_routing_layer
    assert build_route_costs_command.add_collect is False
    assert tuple(build_route_costs_command.preprocessor_args) == ("config",)


@pytest.mark.skipif(
    (os.environ.get("TOX_RUNNING") == "True")
    and (platform.system() == "Windows"),
    reason="CLI does not work under tox env on windows",
)
def test_cli_route_points_flip_start_end(
    cli_runner, sample_layered_data, tmp_path
):
    """route-points CLI that internally flips start and end points"""

    to_test_routes = [
        # More unique endpoints than start points,
        # so computation will flip them
        ((1, 1), (2, 3)),
        ((1, 1), (3, 4)),
        ((1, 1), (4, 5)),
        ((1, 2), (5, 6)),
        ((1, 3), (5, 6)),
    ]

    routes = _build_route_table(
        sample_layered_data,
        rows_cols=to_test_routes,
    )
    route_table_fp = tmp_path / "route_points.csv"
    routes.to_csv(route_table_fp, index=False)

    config = {
        "cost_fpath": str(sample_layered_data),
        "route_table": str(route_table_fp),
        "cost_layers": [{"layer_name": "layer_1"}],
    }
    config_fp = tmp_path / "route_points_config.json"
    config_fp.write_text(json.dumps(config))

    first_result = cli_runner.invoke(
        main, ["route-points", "-c", str(config_fp)]
    )
    assert first_result.exit_code == 0, first_result.output

    out_fp = list(tmp_path.glob("*test*.csv"))
    assert len(out_fp) == 1
    out_fp = out_fp[0]
    assert out_fp.exists()

    output = pd.read_csv(out_fp)
    assert len(output) == len(routes)
    for route_id, route in enumerate(to_test_routes):
        (start_row, start_col), (end_row, end_col) = route

        output_route = output.loc[
            output["route_id"] == f"route_{route_id}"
        ].iloc[0]

        assert output_route["start_row"] == start_row
        assert output_route["start_col"] == start_col
        assert output_route["end_row"] == end_row
        assert output_route["end_col"] == end_col


if __name__ == "__main__":
    pytest.main(["-q", "--show-capture=all", Path(__file__), "-rapP"])
